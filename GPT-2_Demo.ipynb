{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Review Generator\n",
    "GPT-2 demostration by Feng Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disclaimer: \n",
    "since we don't have permission to install packages on Cheaha directly, we created a virtual environment to install all the required packages locally under jwang96's home directory, and linked this virtual environment to Jupyter as tf-local. To remain the same developing environment and for the ease of debugging, we developed our code under jwang96's Cheaha account, and use the same dataset under jwang96's scratch directory.\n",
    "\n",
    "#### Warning: Restarting the kernel is required once run the model 'gpt-books' or 'gpt-elec'. gpt2 can load only one model each time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. One sample from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"overall\": 5.0, \"verified\": false, \"reviewTime\": \"03 30, 2005\", \"reviewerID\": \"A1REUF3A1YCPHM\", \"asin\": \"0001713353\", \"style\": {\"Format:\": \" Hardcover\"}, \"reviewerName\": \"TW Ervin II\", \"reviewText\": \"The King, the Mice and the Cheese by Nancy Gurney is an excellent children's book.  It is one that I well remember from my own childhood and purchased for my daughter who loves it.\\n\\nIt is about a king who has trouble with rude mice eating his cheese. He consults his wise men and they suggest cats to chase away the mice. The cats become a nuisance, so the wise men recommend the king bring in dogs to chase the cats away.  The cycle goes on until the mice are finally brought back to chase away the elephants, brought in to chase away the lions that'd chased away the dogs.\\n\\nThe story ends in compromise and friendship between the mice and the king.  The story also teaches cause and effect relationships.\\n\\nThe pictures that accompany the story are humorous and memorable.  I was thrilled to discover that it is back in print.  I *highly* recommend it for children ages 2 to 7.\", \"summary\": \"A story children will love and learn from\", \"unixReviewTime\": 1112140800}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = '/data/scratch/jwang96/Books_5.json'\n",
    "with open(data_file, 'r') as f:\n",
    "    print(f.readline())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Dataset (1M reviews on Books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We only need the score(overall) and text(reviewText) of each record, so we deal with raw data to write our own training data. Meanwhile, we add ' <|endoftext|>\\n\\n' between each record, which will make our work more manageable later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_file = '/data/scratch/jwang96/books.large'\n",
    "def extract_corpus(data_file, n):\n",
    "    corpus = \"\"\n",
    "    with open(data_file, 'r') as f:\n",
    "        for i in range(n):\n",
    "            line = f.readline()\n",
    "            try:\n",
    "                js = json.loads(line)\n",
    "                revtxt = js['reviewText']\n",
    "                score = js['overall']\n",
    "                revtxt = revtxt.replace('\\n', ' ')\n",
    "                corpus += str(score)+': '+revtxt\n",
    "                corpus += ' <|endoftext|>\\n\\n'\n",
    "            except:\n",
    "                continue\n",
    "    return corpus\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(extract_corpus(data_file, 1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warning: Restarting the kernel is required if run the model 'gpt-elec'\" before. gpt2 can load only one model each time.\n",
    "* This Fine-tune the model is based on 124M pretrained model, which is download from https://github.com/minimaxir/gpt-2-simple.\n",
    "* The training code is in GPT2_train.py file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1201 21:04:32.874630 46912496440576 deprecation.py:323] From /home/jwang96/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/gpt-books/model-1000\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt-books'\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess, run_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example output:\n",
    "\n",
    "### 4.1 General Review Text\n",
    "* Generates review text from Ranking 1 to 5 star.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0: It is fun to talk about the history of the Indians in the early 1900's.  I have written more than once related to the lighthouse and the life of the woman who was once there.  I have also read about the Indian tribes that lived in the South during the early 1900's.  I love the fact that the author has a background in the history of the South.  It is not only interesting to read about the settlers of the South but also to hear about the history of the Indians.  I highly recommend this book to anyone interested in the history of the South. \n",
      "2.0: I was also given a copy of the book by the author.  This is a book that I recommend to people who are curious about the Bible. \n",
      "3.0: If you have been through the Heckman's Bible before, you will have read this book.  How else to explain how the Bible was written and what its meaning is.  I read this book once a year on a Sunday afternoon and by the next morning, I found it a home.  I had never read it, so I decided to read it again.  I had never read it before and I figured, this would be the one.  I was a bit surprised at the amount of responses it received.  I find it refreshing that the Bible is used to sing about how to do God's will.  It's getting a bit stale in this day and age, but I found that the Bible is a very meaningful and powerful tool for learning to practice the will.  Not just for the Bible-teacher, right?  For other Bible-teachers, this book is a great tool for helping them become more aware of the nature and effects of their faith.  I found that the Bible was very helpful to me and I have not used it on any other topic in my church.  She also said that some of the Bible's literature was not taught in the bible, but the Bible is not the bible of the Bible.  I think that this book is a perfect introduction to the Bible.  I'm sure you will enjoy this book.  I've read many of the books about God's will and my understanding of God's will has not changed.  I think this book is a must read for anyone who is interested in God's will and in the nature of God's will. \n",
      "4.0: I really enjoyed this book. It has a high level of detail but it is also very hard to follow and there is confusion about what everyone is doing. I am sure this is a book that you will want to read again and again. \n",
      "5.0: The story takes place during the early days of the Catholic Church and its twenty-first century movement, but the real story is the historical story of the reformation of the Roman Catholics, its impact on the world and the world's history. \n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, run_name=model_name, prefix='1.0', truncate='<|endoftext|>')\n",
    "gpt2.generate(sess, run_name=model_name, prefix='2.0', truncate='<|endoftext|>')\n",
    "gpt2.generate(sess, run_name=model_name, prefix='3.0', truncate='<|endoftext|>')\n",
    "gpt2.generate(sess, run_name=model_name, prefix='4.0', truncate='<|endoftext|>')\n",
    "gpt2.generate(sess, run_name=model_name, prefix='5.0', truncate='<|endoftext|>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Specific Review Text\n",
    "* Generates review text from specific products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0: The Harry Potter author has created a perfect world that is so unique.  The characters are so unique and so original that they bring out the food at the table.  Your children will love the books.  This book is a must read! \n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, run_name=model_name, prefix='5.0: The Harry Potter', truncate='<|endoftext|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0: The Twilight Zone is the second best book of all time.  It's the new kind of medium.  It's the same old medium that plays out in movies.  It's the same old medium that weaves into the music of the 1950s.  And the results are amazing.  The Twilight Zone is the one book with the signal to the world that the world has turned into a movie.  Will this be the next book?  I don't know.  It's too good to be true and I wish I had read it years earlier.  I'm not a fan of the movie, but it's still a great book. \n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, run_name=model_name, prefix='5.0: The Twilight', truncate='<|endoftext|>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis:\n",
    "* These outputs are very human-like reviews and the content matches the given suffix. \n",
    "* Outputs are very similar between some reviews no matter what name of books we are generated. \n",
    "    - Maybe it is because in our dataset, in reviewing book product review, people are often not including too much plot-related content (maybe trying not to be a \"spoiler\"). Therefore most of the reviews praise the product in a very same way (\"interesting\", \"funny,\" etc.).\n",
    "* The model is terrible at discriminating reviews between low and high scores. \n",
    "    - 5.0 review is not bad, while 1.0 review does not make sense. \n",
    "    - The incorrectness of low score review might because low score reviews are insufficient. Only a few generated-reviews get a low score. It does make sense from the experience of our life. We used to mark full-score as long as the products are not horrible.\n",
    "    - The reviews of the 5.0 score have a higher probability of being a short sequence. Because in the training data, a massive bunch of 5.0-score reviews is concise and brief.\n",
    "* With the help of Grammarly, the scores of 4 examples are 62, 96, 90, and 78 correspondingly. \n",
    "    - There are many tiny grammar mistakes. e.g., \"its\" should be \"it's.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model fine-funed from Electronic dataset\n",
    "\n",
    "#### Warning: Restarting the kernel is required if run the model 'gpt-elec'\" before. gpt2 can load only one model each time.\n",
    "\n",
    "\n",
    "\n",
    "* We also fine-tuned another categories which is Electronic to see if we can get some different results.\n",
    "* Restarting the kernel is required if run the \"model_name = 'gpt-books'\" before. gpt2 can load only one model each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1201 21:07:43.541576 46912496440576 deprecation.py:323] From /home/jwang96/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/gpt-elec/model-1000\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt-elec' # fine-tuned from Electronic.json\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess, run_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0: I have had this for over a year and it works perfectly.  I have had several other products.  This one works great as well.  It comes with a case, instructions and a usb mouse.  It works with my 1st gen mouse.  Its a little heavier than I expected, but I still use mine for the most part.  I use it for school.  Its a good deal. \n",
      "2.0: this is great. it does well for my needs, and the Mounts are the best it came with. \n",
      "3.0: I bought this for my Nikon D7000 and it works perfectly.  You can change settings and it's easy to set.  There are a couple of things that I really like: 1) it's small and compact enough to carry with you and it doesn't interfere with the camera's normal size. 2) It comes with a couple of lenses for the lens.  Once you figure it out, you can use the second lens for portraits and it's not really as big as the first one.  I would say that the second lens is pretty good and I got it for $5.  Again, my recommendation is that buy this for your D7000 and if you need it you buy it.  I was also skeptical that the camera would function these days.  I'm very happy with the camera and the room that it fits into.  I also use the camera for some photo editing and I'm happy to have it.  I don't think the lack of a lens is a big deal but you're in a hurry to get into a scene and get the shots you want.  So I'll leave that aside for now. \n",
      "4.0: I am a big fan of this product.  I am using it to connect 5 different computers.  For the price and the sound quality it can't be beat. \n",
      "5.0: I have always liked the sound of the JBLs for the money.  They are very crisp and clear.  They are very portable and come with all the hardware needed to get them out the door of a car.  They are also very good for the price. \n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, run_name=model_name, prefix='1.0', truncate='<|endoftext|>')\n",
    "gpt2.generate(sess, run_name=model_name, prefix='2.0', truncate='<|endoftext|>')\n",
    "gpt2.generate(sess, run_name=model_name, prefix='3.0', truncate='<|endoftext|>')\n",
    "gpt2.generate(sess, run_name=model_name, prefix='4.0', truncate='<|endoftext|>')\n",
    "gpt2.generate(sess, run_name=model_name, prefix='5.0', truncate='<|endoftext|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0: The Kindle Fire II is awesome. I am happy with it. You can even turn it up to edit photos and videos. I can even use the Music player. \n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, run_name=model_name, prefix='5.0: The Kindle', truncate='<|endoftext|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0: This Camera is great for traveling. I have been using this camera for a month now and the pictures are so good. I use it with the Canon 17-55mm and have not had any problems. \n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, run_name=model_name, prefix='5.0: This Camera', truncate='<|endoftext|>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis:\n",
    "* Similar to result of \"fine-tuned model with books dataset\".\n",
    "* The content of reviews become more specific because electronic products have greater differences than books.\n",
    "* The model is still terrible at discriminating reviews between low and high scores. \n",
    "* the scores evaluated by Grammarly are good but some of them get low score due to grammar mistakes. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. Summary\n",
    "\n",
    "  In conclusion, GPT-2 does well in the text generation process. The relatedness of content is great. And it's clear to see the reviews are talking about different products if we changed the training dataset.The generated text is readable and easy to understand. It could generate precisely content with prefix provided, although some sentences get low scores with Grammarly due to tiny grammar mistakes (e.g., its should be it's). However, the performance on discrimination of scores is terrible. We think this is because 1.0 reviews are insufficient in training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-tf",
   "language": "python",
   "name": "local-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
